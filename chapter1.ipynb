{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Self-play\n",
    "\n",
    "* *What if the RL agent plays vs random opponent?*\n",
    "* *Would it learn a different way of playing?*\n",
    "\n",
    "Self-play is a great way for a bot to learn, I believe. If our algorithm can obtain the optimal policy from playing with any opponent, it will learn to play with itself also. It's like [minimax](https://en.wikipedia.org/wiki/Minimax): you suppose that the opponent plays optimally, but you do not care if it does not. \n",
    "\n",
    "Not sure if it's corret, but I believe that if opponent is not so good, the RL agent will also play on low level, but if it plays with itself, it always imroves his own level of play by challenging itself. Interesting question: will it learn to defend against its own tactic of attack?\n",
    "\n",
    "Another interesting point: playing with itself looks like [curriculum learning](http://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf): you first give it simple examples, then they are getting harder and harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Symmetries\n",
    "\n",
    "* *How can we exploit the symmetry when training the RL agent for tic-tac-toe?*\n",
    "* *What if the opponent does not care about the symmetry but we do?*\n",
    "* *Do symmetrically equivalent positions should have the same values?*\n",
    "\n",
    "In order to exploit the symmetry, we can add pre-processing of a state before giving it to an agent: e.g. preproc(state) will return state with top-left corner tic for 4 states where the tic is in any corner (other cells are empty) etc.\n",
    "\n",
    "If the opponent does not care about the symmetry it can explore different tactics in different but symmetrical positions. It can be good in terms of exploration. Hence, it's not always a good idea to exploit the symmetry (if we do not care about the training time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Greedy Play\n",
    "\n",
    "* *What if RL agent is greedy?*\n",
    "* *Would it learn to play better/worse than a non-greedy one?*\n",
    "* *What problems mught occur?*\n",
    "\n",
    "If the agent is greedy, we always do [exploitation](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf) and might never find better moves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4: Learning from Exploration\n",
    "\n",
    "* *Suppose learning updates occured every move even after exploratory. If step-size is reduced over time, then the state values would converge to a set of probabilities. What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves?*\n",
    "* *Which set is better to learn?*\n",
    "* *Which would result in more wins?*\n",
    "\n",
    "I'm not sure but will we learn the transitions to the exploratory branches of our MDP? Yes, we will learn how to behave optimally inside of them, but how will we know that we should go into them?\n",
    "\n",
    "I do not fully get the difference between the two last questions: what is better to learn and which would result in more wins? If my first assumption is correct, we should to training after exploratory moves and learn hot to get there to obtain better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Other Improvements\n",
    "\n",
    "* *Can you think of other ways to improve the RL player?*\n",
    "* *Any better solution to tic-tac-toe problem?*\n",
    "\n",
    "No ideas yet, but [this](https://arxiv.org/abs/1606.03476) is something that is worth thinking about: learning from expert game without any reward function or even without interaction with the expert."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
